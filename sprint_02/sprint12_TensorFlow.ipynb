{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sprint 12 　　　ディープラーニング　フレームワーク  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】スクラッチを振り返る  \n",
    "１．重みを初期化する。  \n",
    "２．学習モデルの決定。  \n",
    "３．フォワード処理、バックワード処理、勾配降下法などで、より適したｗやｂを導出・更新。  \n",
    "４．上記の２と３の処理を繰り返す（イタレーション、エポック）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】スクラッチとTensorFlowの対応を考える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request as req\n",
    "# import pandas as pd\n",
    "\n",
    "# # ファイルをダウンロードする\n",
    "# url = \"https://raw.githubusercontent.com/pandas-dev/pandas/master/pandas/tests/data/iris.csv\" # 先ほどのURLではない\n",
    "# savefile = \"Iris.csv\"\n",
    "# req.urlretrieve(url, savefile)\n",
    "\n",
    "# # ダウンロードしたファイルの内容を表示する\n",
    "# csv = pd.read_csv(savefile, encoding=\"utf-8\")\n",
    "# csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 0.1672, val_loss : 0.7845, acc : 1.000, val_acc : 0.938\n",
      "Epoch 1, loss : 18.3028, val_loss : 46.0440, acc : 0.750, val_acc : 0.375\n",
      "Epoch 2, loss : 2.2667, val_loss : 3.2956, acc : 0.750, val_acc : 0.812\n",
      "Epoch 3, loss : 0.0000, val_loss : 0.1466, acc : 1.000, val_acc : 0.938\n",
      "Epoch 4, loss : 0.0000, val_loss : 0.4751, acc : 1.000, val_acc : 0.938\n",
      "Epoch 5, loss : 0.0000, val_loss : 0.8386, acc : 1.000, val_acc : 0.938\n",
      "Epoch 6, loss : 0.0000, val_loss : 0.5987, acc : 1.000, val_acc : 0.875\n",
      "Epoch 7, loss : 0.0000, val_loss : 0.0022, acc : 1.000, val_acc : 1.000\n",
      "Epoch 8, loss : 0.0000, val_loss : 0.2404, acc : 1.000, val_acc : 0.938\n",
      "Epoch 9, loss : 0.0000, val_loss : 0.2433, acc : 1.000, val_acc : 0.938\n",
      "test_acc : 0.900\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Name\"] == \"Iris-versicolor\")|(df[\"Name\"] == \"Iris-virginica\")]\n",
    "y = df[\"Name\"]\n",
    "X = df.loc[:, [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "\n",
    "####　X placeholderに入力するデータ次元数 ４  ####\n",
    "####　X placeholderに入力するデータサンプル数　１００   ####\n",
    "####　Y placeholderに入力するクラス次元数 １   ####\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み\n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・スクラッチにおけるＦｉｔ関数と、ＴＦにおけるネットワーク構造の読み込み（example_net）が対応していそう。  \n",
    "・ＴＦでは、バックプロパゲーションの記述がない。  \n",
    "・ＴＦでは、初期化がどのようにされているか不明。    \n",
    "・ＴＦでは、最適化の際に、目的関数をminimizeするという指針がある（ネットワーク構造の最後の活性化関数が、最適化処理と同時になる）。  \n",
    "・スクラッチではaccをsklearnで計算。ＴＦでは、accを自力で計算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "##############  データフレームから条件抽出  ############## \n",
    "df = df[(df[\"Name\"] == \"Iris-setosa\")|(df[\"Name\"] == \"Iris-versicolor\")|(df[\"Name\"] == \"Iris-virginica\")]\n",
    "y = df[\"Name\"]\n",
    "X = df.loc[:, [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "##############      ラベルを数値変換        ############## \n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "\n",
    "# y = y.astype(np.int)[:, np.newaxis]\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_one_hot = enc.fit_transform(y[:,np.newaxis])\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 11.1998, val_loss : 28.1083, acc : 0.500, val_acc : 0.583\n",
      "Epoch 1, loss : 17.9112, val_loss : 17.4121, acc : 0.750, val_acc : 0.583\n",
      "Epoch 2, loss : 4.5029, val_loss : 17.3537, acc : 0.750, val_acc : 0.667\n",
      "Epoch 3, loss : 1.5907, val_loss : 10.8115, acc : 0.833, val_acc : 0.667\n",
      "Epoch 4, loss : 6.2686, val_loss : 6.0041, acc : 0.833, val_acc : 0.792\n",
      "Epoch 5, loss : 0.3766, val_loss : 11.7053, acc : 0.917, val_acc : 0.708\n",
      "Epoch 6, loss : 1.4234, val_loss : 4.0520, acc : 0.917, val_acc : 0.833\n",
      "Epoch 7, loss : 1.0658, val_loss : 3.2898, acc : 0.917, val_acc : 0.833\n",
      "test_acc : 0.933\n"
     ]
    }
   ],
   "source": [
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 12\n",
    "num_epochs = 8\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "\n",
    "###########　Y placeholderに入力するクラス次元数 3   ##########\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None,n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み\n",
    "logits = example_net(X)\n",
    "#############       目的関数        ################\n",
    "#loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y,logits = logits)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y,logits = logits))\n",
    "\n",
    "#最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "###############       推定結果       ###############\n",
    "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            # print(sess.run(temp, feed_dict={X: mini_batch_x, Y: mini_batch_y}))\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】MNISTのモデルを作成  　(問題４は後述)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "X_test.shape\n",
    "y_test_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 0.3889, val_loss : 1.9250, acc : 0.900, val_acc : 0.781\n",
      "Epoch 1, loss : 0.2876, val_loss : 1.5650, acc : 0.950, val_acc : 0.814\n",
      "Epoch 2, loss : 0.1109, val_loss : 1.4275, acc : 0.950, val_acc : 0.827\n",
      "Epoch 3, loss : 0.0274, val_loss : 1.3493, acc : 1.000, val_acc : 0.832\n",
      "Epoch 4, loss : 0.0292, val_loss : 1.3142, acc : 1.000, val_acc : 0.835\n",
      "Epoch 5, loss : 0.0323, val_loss : 1.2770, acc : 1.000, val_acc : 0.836\n",
      "Epoch 6, loss : 0.0345, val_loss : 1.2564, acc : 1.000, val_acc : 0.837\n",
      "Epoch 7, loss : 0.0362, val_loss : 1.2385, acc : 1.000, val_acc : 0.838\n",
      "Epoch 8, loss : 0.0390, val_loss : 1.2239, acc : 1.000, val_acc : 0.839\n",
      "Epoch 9, loss : 0.0410, val_loss : 1.2118, acc : 1.000, val_acc : 0.840\n",
      "Epoch 10, loss : 0.0418, val_loss : 1.2023, acc : 1.000, val_acc : 0.841\n",
      "Epoch 11, loss : 0.0417, val_loss : 1.1922, acc : 1.000, val_acc : 0.844\n",
      "Epoch 12, loss : 0.0426, val_loss : 1.1833, acc : 1.000, val_acc : 0.844\n",
      "Epoch 13, loss : 0.0422, val_loss : 1.1746, acc : 1.000, val_acc : 0.844\n",
      "Epoch 14, loss : 0.0414, val_loss : 1.1682, acc : 1.000, val_acc : 0.845\n",
      "Epoch 15, loss : 0.0431, val_loss : 1.1614, acc : 1.000, val_acc : 0.845\n",
      "Epoch 16, loss : 0.0443, val_loss : 1.1555, acc : 1.000, val_acc : 0.845\n",
      "Epoch 17, loss : 0.0454, val_loss : 1.1509, acc : 1.000, val_acc : 0.846\n",
      "Epoch 18, loss : 0.0457, val_loss : 1.1468, acc : 1.000, val_acc : 0.847\n",
      "Epoch 19, loss : 0.0454, val_loss : 1.1432, acc : 1.000, val_acc : 0.846\n",
      "test_acc : 0.855\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "learning_rate = 0.025\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "n_hidden1 = 400   ### 以前のNNと同様に設定\n",
    "n_hidden2 = 200   ### 以前のNNと同様に設定\n",
    "\n",
    "n_input = X_train.shape[1]   ### 784\n",
    "n_samples = X_train.shape[0]   ### X_trainなら48000\n",
    "n_classes = 10    ### クラス数１０\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None,n_classes])\n",
    "\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.tanh(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.tanh(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "logits = example_net(X)\n",
    "#############       目的関数        ################\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y,logits = logits))\n",
    "\n",
    "#最適化手法\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "###############       推定結果       ###############\n",
    "correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(Y,1))\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            # print(sess.run(temp, feed_dict={X: mini_batch_x, Y: mini_batch_y}))\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】House Pricesのモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import IPython\n",
    "import numpy as np\n",
    "\n",
    "aims_data = pd.read_csv(\"train.csv\")\n",
    "aims_data4 = aims_data.loc[:,['GrLivArea','YearBuilt','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = aims_data4.iloc[:,0:2].values\n",
    "y = aims_data4.iloc[:,2].values\n",
    "X = np.log(X)\n",
    "y = np.log(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state = 1)\n",
    "y_train=y_train[:,np.newaxis]\n",
    "y_test=y_test[:,np.newaxis]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size=0.25, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 1) (292, 1) (292, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 11727.5430, val_loss : 9706.5908\n",
      "Epoch 1, loss : 68.4588, val_loss : 110.0960\n",
      "Epoch 2, loss : 10.9124, val_loss : 10.3980\n",
      "Epoch 3, loss : 4.0550, val_loss : 3.9975\n",
      "Epoch 4, loss : 12.2360, val_loss : 17.7899\n",
      "Epoch 5, loss : 11.1419, val_loss : 16.3145\n",
      "Epoch 6, loss : 28.7590, val_loss : 35.3295\n",
      "Epoch 7, loss : 15.6917, val_loss : 20.9770\n",
      "Epoch 8, loss : 21.9022, val_loss : 26.5628\n",
      "Epoch 9, loss : 2.0190, val_loss : 3.9053\n",
      "test_loss : 3.485\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "#####  tensorboard のための記憶フォルダ作成　#####\n",
    "log_dir = 'our_graph'\n",
    "if tf.gfile.Exists(log_dir):\n",
    "    tf.gfile.DeleteRecursively(log_dir)\n",
    "tf.gfile.MakeDirs(log_dir)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 20\n",
    "num_epochs = 10\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 50\n",
    "\n",
    "###########　Y placeholderに入力するクラス次元数 3   ##########\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None,n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_hidden3])),\n",
    "        'w4': tf.Variable(tf.random_normal([n_hidden3, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden3])),\n",
    "        'b4': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    layer_output = tf.matmul(layer_3, weights['w4']) + biases['b4'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み\n",
    "logits = example_net(X)\n",
    "#############       目的関数        ################\n",
    "#loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y,logits = logits)\n",
    "loss_op = tf.reduce_mean(tf.square(logits - Y))\n",
    "\n",
    "#最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ###############       推定結果       ###############\n",
    "# #correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# correct_pred = logits\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "\n",
    "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_loss : {:.3f}\".format(test_loss))\n",
    "    \n",
    "    #####  tensorboard のためのファイル作成　#####\n",
    "    summary_writer = tf.summary.FileWriter(log_dir , sess.graph)\n",
    "    #####　anacondaプロンプトで、tensorboard --logdir=/tmp/・・・/our_graph\n",
    "    #####  http://localhost:6006/ を開く"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
