{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sprint10 畳み込みニューラルネットワーク_１ｄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成  \n",
    "## 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "　問題2は、class SimpleConv1dにおけるdef out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_columns', 250)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_test y_testは使用しない？　###\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "# print(y_train.shape) # (60000,)\n",
    "# print(y_train_one_hot.shape) # (60000, 10)\n",
    "# print(y_train_one_hot.dtype) # float64\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self,x):\n",
    "#          out = 1/(1 + np.exp(-x))\n",
    "        out = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx = dout * (1 - self.out**2) ### 確認。out?\n",
    "#         dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    def forward(self,x):\n",
    "        self.y = np.exp(x)/ (np.sum(np.exp(x)).reshape(-1,1))\n",
    "        return self.y\n",
    "    def backward(self,t):\n",
    "        self.t = t\n",
    "        delta_sf = self.y -self.t\n",
    "#       self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return delta_sf\n",
    "    def cross_entropy_error(self,y,t):\n",
    "        self.t = t\n",
    "        c_entropy = -1/len(self.t)*(np.sum(self.t*np.log(self.y))) ### ｔは1hot_encoder\n",
    "        return c_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        self.initializer = initializer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2        \n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = self.initializer.WI(self.n_nodes1,self.n_nodes2)\n",
    "        self.B = self.initializer.BI(self.n_nodes2)\n",
    "\n",
    "    def forward(self, Z):\n",
    "        self.Z = Z.copy()  ### copyは不要かもしれない。\n",
    "        A = np.dot(self.Z, self.W)+ self.B\n",
    "        return A\n",
    "  \n",
    "    def backward(self, dA):\n",
    "        self.dB = dA\n",
    "        self.dW = np.dot(self.Z.reshape(-1,1), dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        self = self.optimizer.update(self) \n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xavier_Initializer:\n",
    "    def __init__():\n",
    "        self.xavier = None\n",
    "    def WI(self, n_nodes1):\n",
    "        self.xavier = 1/math.sqrt(n_nodes1)\n",
    "        WI = np.random.randn(n_nodes1) * self.xavier\n",
    "        return WI\n",
    "    def BI(self, n_nodes2):\n",
    "        BI = np.random.randn(n_nodes2) * self.xavier\n",
    "        return BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def WI(self, n_nodes1, n_nodes2):\n",
    "        WI = np.random.randn(n_nodes1, n_nodes2) * self.sigma\n",
    "        return WI\n",
    "    def BI(self, n_nodes2):\n",
    "        BI = np.random.randn(n_nodes2) * self.sigma\n",
    "        return BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch_1d_CNN_Classifier:\n",
    "    def __init__(self, verbose = True, iter_num=1, lr=0.002, sigma=0.02):\n",
    "        self.verbose = verbose\n",
    "        self.iter_num = iter_num\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def fit(self,X,y):\n",
    "\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_nodes1 = 782\n",
    "        self.n_nodes2 = 200\n",
    "        self.n_output = 10\n",
    "        # 全結合層と畳み込み層でoptimizerを変えているのは、バイアス項のshapeに差異があるため\n",
    "        optimizer1 = SGD(self.lr)\n",
    "        optimizer2 = SGD2(self.lr)\n",
    "\n",
    "        self.FC1 = SimpleConv1d(Xavier_Initializer, optimizer1)\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(0.02), optimizer2)\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(0.02), optimizer2)\n",
    "        self.activation3 = Softmax()\n",
    "\n",
    "        for i in range(self.iter_num):\n",
    "             for mini_X_train, mini_y_train in zip(X,y):\n",
    "               # print(mini_X_train.shape)\n",
    "                ### forward\n",
    "                self.A1 = self.FC1.forward(mini_X_train)\n",
    "                #print(self.A1.shape)\n",
    "                self.Z1 = self.activation1.forward(self.A1)\n",
    "                #print(self.Z1.shape)\n",
    "                self.A2 = self.FC2.forward(self.Z1)\n",
    "                #print(self.A2.shape)\n",
    "                self.Z2 = self.activation2.forward(self.A2)\n",
    "                #print(self.Z2.shape)\n",
    "                self.A3 = self.FC3.forward(self.Z2)\n",
    "                #print(self.A3.shape)\n",
    "                self.Z3 = self.activation3.forward(self.A3)\n",
    "\n",
    "                ### backward\n",
    "                self.delta1 = self.activation3.backward(mini_y_train)\n",
    "                #print(self.delta1.shape)\n",
    "                self.dZ2 = self.FC3.backward(self.delta1)\n",
    "                #print(self.Z2.shape)\n",
    "                self.delta2 = self.activation2.backward(self.dZ2)\n",
    "                self.dZ1 = self.FC2.backward(self.delta2)\n",
    "                self.delta3 = self.activation1.backward(self.dZ1)\n",
    "                self.dZ3 = self.FC1.backward(self.delta3)\n",
    "                \n",
    "                #print(c_entropy)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        pred_array = np.zeros(X.shape[0])\n",
    "        count = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            mini_X = X[i,:]\n",
    "\n",
    "            PA1 = self.FC1.forward(mini_X)\n",
    "            PZ1 = self.activation1.forward(PA1)\n",
    "            PA2 = self.FC2.forward(PZ1)\n",
    "            PZ2 = self.activation2.forward(PA2)\n",
    "            PA3 = self.FC3.forward(PZ2)\n",
    "            PZ3 = self.activation3.forward(PA3)\n",
    "            pred_array[count] = np.argmax(PZ3,axis=1)\n",
    "            count = count + 1\n",
    "        return pred_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.02):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        layer.W = layer.W - self.lr * layer.dW\n",
    "        layer.B = layer.B - self.lr * layer.dB\n",
    "#        layer.B = layer.B - self.lr * layer.dB.mean(axis=0)\n",
    "\n",
    "#        print(layer.W.shape,layer.B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD2:\n",
    "    def __init__(self, lr=0.02):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        layer.W = layer.W - self.lr * layer.dW\n",
    "        layer.B = layer.B - self.lr * layer.dB.mean(axis=0)\n",
    "\n",
    "#        print(layer.W.shape,layer.B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    ## Z = (1,748)\n",
    "    \n",
    "    def __init__(self, initializer=Xavier_Initializer, optimizer = SGD() ):\n",
    "        #self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.F = 3\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = self.initializer.WI(self, self.F)  ### ＷＩの引数にselfがいるのはなぜか。\n",
    "        self.B = self.initializer.BI(self, 1)\n",
    "        self.dW = np.zeros(self.F)\n",
    "        self.dB = np.zeros(1)\n",
    "        \n",
    "        # paddingとstrideは0と1で固定\n",
    "        self.padding_size =  0\n",
    "        self.stride_size = 1\n",
    "\n",
    "    def out_size(self):\n",
    "        n_out = (self.input_size + self.padding_size*2 - self.F)//self.stride_size + 1\n",
    "        self.output_size = int(n_out)\n",
    "        return int(n_out)\n",
    "        \n",
    "    def forward(self, Z):\n",
    "        self.Z = Z\n",
    "\n",
    "        self.input_size = self.Z.shape[0]\n",
    "        A = np.zeros(self.out_size())\n",
    "        for i in range(self.output_size):\n",
    "                # broad cast 注意。\n",
    "                #print(np.dot(self.Z[bat,i:i+self.F],self.W).shape)\n",
    "            A[i] = np.dot(self.Z[i:i+ self.F], self.W)  + self.B\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        self.dB = np.sum(dA)\n",
    "#        print(dA.shape)\n",
    "        for bat in range(self.Z.shape[0]):\n",
    "            for i in range(len(self.dW)):\n",
    "#                dw[output_c][input_c] += a[input_c][h:self.W.shape[2]+h, w:self.W.shape[3]+w] * dA[output_c][h][w]\n",
    "\n",
    "                temp = self.Z[i:i + len(dA)].reshape(1,-1)\n",
    "                self.dW[i] = np.sum(np.dot(temp, dA))\n",
    "        \n",
    "        self.dA_array = np.zeros((self.input_size, len(self.W)))\n",
    "        \n",
    "#        print(\"dAshape\")\n",
    "#        print(self.dW.shape,self.W.shape)\n",
    "\n",
    "        for j in range(self.input_size):\n",
    "            for s in range(len(self.W)):\n",
    "                if j-s < 0 or (j-s > self.output_size - 1): \n",
    "                    self.dA_array[j][s] = 0\n",
    "                else:\n",
    "                    self.dA_array[j][s] = dA.T[j-s]\n",
    "        \n",
    "        dZ = np.dot(self.dA_array, self.W)\n",
    "        self.delta=dZ\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return  dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】学習と推定  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-ba84bc56570a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScratch_1d_CNN_Classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-f1ba63d82182>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdZ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdZ3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;31m#print(c_entropy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-204-c8f37dc9b939>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dA)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdA_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = Scratch_1d_CNN_Classifier()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### trainデータが18000サンプル時（splitの、test_size=0.7時）の　accyracy 結果　###\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test[0:500], clf.predict(X_test[0:500,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  【問題3】小さな配列での1次元畳み込み層の実験  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 [ 50.  80. 110.] [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "clf_temp = SimpleConv1d()\n",
    "x = np.array([1, 2, 3, 4])\n",
    "clf_temp.W =  np.array([3, 5, 7])\n",
    "\n",
    "dA = np.array([10, 20])\n",
    "clf_temp.forward(x)\n",
    "clf_temp.backward(dA)\n",
    "print(clf_temp.dB,clf_temp.dW,clf_temp.delta)\n",
    "\n",
    "##################################################################################\n",
    "### SimpleConv1dのinitでは、SGDではなくSGD() と書く必要がある。\n",
    "### SGD（）と書くことでインスタンス化され、SGDのみだとインスタンス化されない。\n",
    "### SGDでは、設計図のみが代入されている状況となる。\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xavier_Initializer2:\n",
    "    def __init__():\n",
    "        self.xavier = None\n",
    "    def WI(self, output_channels_num, input_channels_num, F):\n",
    "        self.xavier = 1/math.sqrt(F)\n",
    "        WI = np.random.randn(output_channels_num, input_channels_num, F) * self.xavier\n",
    "        return WI\n",
    "    def BI(self, output_channels_num):\n",
    "        BI = np.random.randn(output_channels_num) * self.xavier\n",
    "        return BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d_multi:\n",
    "    def __init__(self, initializer=Xavier_Initializer2, optimizer = SGD2(), input_channels_num=2, output_channels_num=3):\n",
    "\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.F = 3\n",
    "        self.W = self.initializer.WI(self,output_channels_num, input_channels_num, self.F) \n",
    "        self.B = self.initializer.BI(self,output_channels_num)\n",
    "        self.dW = np.zeros((output_channels_num, input_channels_num, self.F))\n",
    "        self.dB = np.zeros(output_channels_num)\n",
    "        \n",
    "        self.padding_size =  0\n",
    "        self.stride_size = 1\n",
    "\n",
    "    def out_size(self):\n",
    "        n_out = (self.input_size + self.padding_size*2 - self.F)//self.stride_size + 1\n",
    "        self.output_size = int(n_out)\n",
    "        return int(n_out)\n",
    "        \n",
    "    def forward(self, Z):\n",
    "        # Z = (入力チャネル数、1d入力)\n",
    "        self.Z = Z\n",
    "        self.input_size = self.Z.shape[1]\n",
    "\n",
    "        self.A = np.zeros([self.W.shape[0], self.out_size()])\n",
    "        # for文　k:出力チャネル、j:入力チャネル、i:1次元フィルタ\n",
    "        for k in range(self.W.shape[0]):\n",
    "            convs = np.zeros([self.Z.shape[0], self.out_size()])     # 入力チャネル数、1d出力情報#\n",
    "            input_sum = np.zeros([1,self.out_size()])                # 入力チャネルの畳み込み合計\n",
    "\n",
    "            for j in range(self.Z.shape[0]):\n",
    "                for i in range(self.out_size()):\n",
    "                    convs[j,i] = np.sum(self.Z[j][i:i+self.F] * self.W[k][j])\n",
    "                \n",
    "            input_sum = np.sum(convs,axis=0)+self.B[k]\n",
    "            self.A[k] = input_sum\n",
    "\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        self.dB = np.sum(dA,axis=1)\n",
    "        self.dZ = np.zeros((self.Z.shape[0], self.Z.shape[1]))\n",
    "        \n",
    "        # for文 input_c:元の入力、output_c:元の出力、i: Wの出力チャネルサイズ数\n",
    "        for input_c in range(self.Z.shape[0]):\n",
    "            for output_c in range(self.W.shape[0]):\n",
    "                for i in range(self.W.shape[0]):\n",
    "                    \n",
    "                    temp = self.Z[input_c][i:i + dA.shape[1]]\n",
    "                    self.dW[output_c][input_c][i] = np.sum(np.dot(temp, dA[output_c]))\n",
    "\n",
    "        # for文　input_c:元の入力、output_c:元の出力、j：Zの1次元入力サイズ、s:1次元フィルタサイズ\n",
    "        for input_c in range(self.Z.shape[0]):\n",
    "            for output_c in range(self.W.shape[0]):\n",
    "                for j in range(self.Z.shape[1]):\n",
    "                    for s in range(len(self.W)):\n",
    "                        if j-s < 0 or (j-s > self.output_size - 1): \n",
    "                            self.dZ[input_c][j] = self.dZ[input_c][j]\n",
    "                        else:\n",
    "                            self.dZ[input_c][j] += dA[output_c][j-s] * self.W[output_c][input_c][s]\n",
    "\n",
    "        delta = self.dZ\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return  delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フォワードプロパゲーションの確認（diverと同様の結果になった。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 22],\n",
       "       [17, 23],\n",
       "       [18, 24]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_temp2 = SimpleConv1d_multi()\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \n",
    "#x = x.reshape(1,-1)\n",
    "clf_temp2.W =  np.ones([3, 2, 3])\n",
    "clf_temp2.B =  np.array([1, 2, 3])\n",
    "\n",
    "clf_temp2.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バックプロパゲーションの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwardの表示\n",
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n",
      "dZ:[[ 50. 119. 119.  69.]\n",
      " [ 50. 119. 119.  69.]]\n",
      "\n",
      "W:[[[-0.18 -0.92 -1.66]\n",
      "  [-0.92 -1.66 -2.4 ]]\n",
      "\n",
      " [[-0.26 -1.06 -1.86]\n",
      "  [-1.06 -1.86 -2.66]]\n",
      "\n",
      " [[-0.32 -1.16 -2.  ]\n",
      "  [-1.16 -2.   -2.84]]]\n",
      "\n",
      "B:[0.20666667 1.20666667 2.20666667]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_temp2 = SimpleConv1d_multi()\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \n",
    "#x = x.reshape(1,-1)\n",
    "clf_temp2.W =  np.ones([3, 2, 3])\n",
    "clf_temp2.B =  np.array([1, 2, 3])\n",
    "\n",
    "d_target = np.array([[1, 0],\n",
    "              [0, 0],\n",
    "              [0, 0]])\n",
    "\n",
    "print(\"forwardの表示\")\n",
    "print(clf_temp2.forward(x))\n",
    "\n",
    "dA = clf_temp2.forward(x)-d_target\n",
    "\n",
    "print(\"dZ:{}\\n\".format(clf_temp2.backward(dA)))\n",
    "print(\"W:{}\\n\".format(clf_temp2.W))\n",
    "print(\"B:{}\\n\".format(clf_temp2.B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
